{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding1D, BatchNormalization, Flatten, Conv1D\n",
    "from keras.layers import AveragePooling1D, MaxPooling1D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "# from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Matt's 'chr1_ukb_X.npz' file\n",
    "chr1_ukb_X = np.load('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X.npz')\n",
    "# create dict mapping from sampleid to superpopulation\n",
    "sampleid_to_superpopulation_pandas_df = pd.read_csv('/home/jsokol/Data/igsr_samples.tsv', delimiter=\"\\t\")\n",
    "sampleid_to_superpopulation_dict = sampleid_to_superpopulation_pandas_df.set_index('Sample name')['Superpopulation code'].to_dict()\n",
    "# remove samples from chr1_ukb_X that are not in dict or that do no contain value\n",
    "temp = set()\n",
    "for x in np.nditer(chr1_ukb_X['S']):\n",
    "    sample_id = str((x.item(0))).replace(\"'\", \"\").replace(\"b\", \"\").split(\"_\")[0]\n",
    "    if sample_id not in sampleid_to_superpopulation_dict:\n",
    "        print(sample_id + ' is not in dict. Make sure to remove this sample from the dataset.')\n",
    "    elif sampleid_to_superpopulation_dict[sample_id] == float('nan'):\n",
    "        print(sample_id + ' maps to nan in dict')\n",
    "chr1_ukb_X_G = chr1_ukb_X['G'][:,:,:]\n",
    "chr1_ukb_X_V = chr1_ukb_X['V'][:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of classes\n",
    "classes = [0, 1, 2, 3, 4]\n",
    "# flatten input data\n",
    "X_all_data = chr1_ukb_X_G.reshape(chr1_ukb_X_G.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode labels\n",
    "Y_all_data = np.zeros((5, 5008))\n",
    "for i in range(5008):\n",
    "    sample_id = str(chr1_ukb_X['S'][i].item(0)).replace(\"'\", \"\").replace(\"b\", \"\").split(\"_\")[0]\n",
    "    if sampleid_to_superpopulation_dict[sample_id] == 'AFR':\n",
    "        Y_all_data[0,i] = 1\n",
    "    elif sampleid_to_superpopulation_dict[sample_id] == 'AMR':\n",
    "        Y_all_data[1,i] = 1\n",
    "    elif sampleid_to_superpopulation_dict[sample_id] == 'EAS':\n",
    "        Y_all_data[2,i] = 1\n",
    "    elif sampleid_to_superpopulation_dict[sample_id] == 'EUR':\n",
    "        Y_all_data[3,i] = 1\n",
    "    elif sampleid_to_superpopulation_dict[sample_id] == 'SAS':\n",
    "        Y_all_data[4,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the order of the data\n",
    "randomize = np.arange(X_all_data.shape[1])\n",
    "np.random.shuffle(randomize)\n",
    "X_all_data = X_all_data[:,randomize]\n",
    "Y_all_data = Y_all_data[:,randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train = X_all_data[:,:4000]\n",
    "X_test = X_all_data[:,4000:]\n",
    "Y_train = Y_all_data[:,:4000]\n",
    "Y_test = Y_all_data[:,4000:]\n",
    "# convert to correct format for use in keras\n",
    "X_train = X_train[:,:,np.newaxis]\n",
    "X_train = np.transpose(X_train, (1, 0, 2))\n",
    "X_test = X_test[:,:,np.newaxis]\n",
    "X_test = np.transpose(X_test, (1, 0, 2))\n",
    "Y_train = np.transpose(Y_train, (1, 0))\n",
    "Y_test = np.transpose(Y_test, (1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(n_snps_per_sample):\n",
    "    \"\"\"\n",
    "    input_shape: The height, width and channels as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(shape=(n_snps_per_sample, 1))\n",
    "\n",
    "    # First convolutional block\n",
    "    X = Conv1D(filters=49, kernel_size=500 ,strides=500, padding = 'valid', name = 'conv0')(X_input)\n",
    "    X = BatchNormalization(axis = -1, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second convolutional block\n",
    "    X = Conv1D(49, 75, strides = 5, padding = 'same', name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = -1, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    X = MaxPooling1D(2, name='max_pool')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(5, activation='softmax', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='model2')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 231504, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv1D)               (None, 463, 49)           24549     \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 463, 49)           196       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 463, 49)           0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 93, 49)            180124    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 93, 49)            196       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 93, 49)            0         \n",
      "_________________________________________________________________\n",
      "max_pool (MaxPooling1D)      (None, 46, 49)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2254)              0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 5)                 11275     \n",
      "=================================================================\n",
      "Total params: 216,340\n",
      "Trainable params: 216,144\n",
      "Non-trainable params: 196\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model2 = model2(X_train.shape[1])\n",
    "# compile model\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize model\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.2545 - accuracy: 0.9305\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.0117 - accuracy: 0.9985\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 7.8973e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 5.1609e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 3.8238e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 3.0450e-04 - accuracy: 1.0000: 0s - loss: 3.0646e-04 - accura\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 2.4660e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 2.0349e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 1.7237e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fecb87e7890>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "1008/1008 [==============================] - 3s 3ms/step\n",
      "test loss, test acc: [0.05440417493529589, 0.9851190447807312]\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = model2.evaluate(X_test, Y_test, batch_size=1)\n",
    "print('test loss, test acc:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
