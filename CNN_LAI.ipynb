{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of cores to 16\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=16, \n",
    "                                                   inter_op_parallelism_threads=16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/magu/deepmix/data/ALL_DNA_dataset/chr1_ALL_X.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3cb86b22fa74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load genetic data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchr1kg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/magu/deepmix/data/ALL_DNA_dataset/chr1_ALL_X.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# S are samples, G are genotypes, V are variants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchr1kg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchr1kg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/magu/deepmix/data/ALL_DNA_dataset/chr1_ALL_X.npz'"
     ]
    }
   ],
   "source": [
    "# load genetic data\n",
    "chr1kg = np.load('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_ALL_X.npz')\n",
    "\n",
    "# S are samples, G are genotypes, V are variants\n",
    "[(i,chr1kg[i].shape) for i in chr1kg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HG00096_S1', 'HG00097_S1', 'HG00099_S1', 'HG00100_S1',\n",
       "       'HG00101_S1'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=chr1kg['S'].astype(str)\n",
    "S[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['21', '9411239', 'G', 'A'],\n",
       "       ['21', '9411245', 'C', 'A']], dtype='<U101')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V=chr1kg['V'].astype(str)\n",
    "V[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2504, 1105538, 14)\n"
     ]
    }
   ],
   "source": [
    "G=chr1kg['G'].astype(bool)\n",
    "print(G.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "chr1kg=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ancestry labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Biosample ID</th>\n",
       "      <th>Population code</th>\n",
       "      <th>Population name</th>\n",
       "      <th>Superpopulation code</th>\n",
       "      <th>Superpopulation name</th>\n",
       "      <th>Population elastic ID</th>\n",
       "      <th>Data collections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>HGDP00982</td>\n",
       "      <td>male</td>\n",
       "      <td>SAMEA3302833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mbuti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Africa (SGDP)</td>\n",
       "      <td>MbutiSGDP</td>\n",
       "      <td>Simons Genome Diversity Project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>Dus22</td>\n",
       "      <td>female</td>\n",
       "      <td>SAMEA3302688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dusun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oceania (SGDP)</td>\n",
       "      <td>DusunSGDP</td>\n",
       "      <td>Simons Genome Diversity Project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>NA13607</td>\n",
       "      <td>male</td>\n",
       "      <td>SAMEA3302722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East Asia (SGDP)</td>\n",
       "      <td>AmiSGDP</td>\n",
       "      <td>Simons Genome Diversity Project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample name     Sex  Biosample ID Population code Population name  \\\n",
       "4969   HGDP00982    male  SAMEA3302833             NaN           Mbuti   \n",
       "4970       Dus22  female  SAMEA3302688             NaN           Dusun   \n",
       "4971     NA13607    male  SAMEA3302722             NaN             Ami   \n",
       "\n",
       "     Superpopulation code Superpopulation name Population elastic ID  \\\n",
       "4969                  NaN        Africa (SGDP)             MbutiSGDP   \n",
       "4970                  NaN       Oceania (SGDP)             DusunSGDP   \n",
       "4971                  NaN     East Asia (SGDP)               AmiSGDP   \n",
       "\n",
       "                     Data collections  \n",
       "4969  Simons Genome Diversity Project  \n",
       "4970  Simons Genome Diversity Project  \n",
       "4971  Simons Genome Diversity Project  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_info=pd.read_csv('/home/jsokol/Data/igsr_samples.tsv', sep=\"\\t\")\n",
    "sample_info.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4019, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## REMOVE AMERICANS + ADMIXED POPS\n",
    "pops_to_remove=['ASW','MXL','CEU','PUR','PEL','CLM','ACB'] # +['GIH','ITU','STU']\n",
    "samples=sample_info[~sample_info['Population code'].isin(pops_to_remove)]\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {'EUR': 0, 'EAS': 1, 'SAS': 2, 'AFR': 3})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=samples[\"Superpopulation code\"].dropna().unique().tolist()\n",
    "labels=dict(zip(labels, range(len(labels))))\n",
    "k=len(labels)\n",
    "(k, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3802, 1105538)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=[i for i in S if i[:-3] in samples['Sample name'].values]\n",
    "S_pop=[str(samples.loc[samples['Sample name']==i[:-3], \"Superpopulation code\"].values[0]) for i in S]\n",
    "Y=np.array([[labels[pop] for _ in range(G.shape[1])] for pop in S_pop])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[np.array([1,345,2222]),:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nope='''Y2=[]\n",
    "for i in range(Y.shape[0]): # individuals\n",
    "    Y2.append([])\n",
    "    for j in range(Y.shape[1]): # sites\n",
    "        Y2[-1].append(np.zeros(k))\n",
    "        Y2[-1][-1][labels[Y[i,j]]]=1\n",
    "Y=np.array(Y2)\n",
    "Y2=[]\n",
    "print(Y.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_nope='''for pop,i in labels.items():\n",
    "    Y[np.where(Y==pop)]=i\n",
    "Y=np.expand_dims(Y, 2)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X_int.train.txt', header=None).iloc[:,0].values\n",
    "dev=pd.read_csv('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X_int.dev.txt', header=None).iloc[:,0].values\n",
    "test=pd.read_csv('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X_int.test.txt', header=None).iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3510,), (152,), (140,)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ix=np.array([i for i,x in enumerate(S) if x in train and x[:-3] in list(samples['Sample name'])])\n",
    "dev_ix=np.array([i for i,x in enumerate(S) if x in dev and x[:-3] in list(samples['Sample name'])])\n",
    "test_ix=np.array([i for i,x in enumerate(S) if x in test and x[:-3] in list(samples['Sample name'])])\n",
    "[train_ix.shape, dev_ix.shape, test_ix.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2504 is out of bounds for axis 0 with size 2504",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ec8b26a57c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2504 is out of bounds for axis 0 with size 2504"
     ]
    }
   ],
   "source": [
    "train_X=G[train_ix,:,:]\n",
    "train_Y=Y[train_ix,:]\n",
    "[train_X.shape, train_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X=G[dev_ix,:,:]\n",
    "dev_Y=Y[dev_ix,:]\n",
    "[dev_X.shape, dev_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X=G[test_ix,:,:]\n",
    "test_Y=Y[test_ix,:]\n",
    "[test_X.shape, test_Y.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment training set with admixed individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of ancestors as Pois(2.86 * generation_time), over 1-10 (maxgen) generations\n",
    "n_fake=4*train_X.shape[0]\n",
    "maxgen=4\n",
    "n_splits=2+np.hstack([ss.poisson.rvs(2.86*gen, size=n_fake//maxgen) for gen in range(1,maxgen)])\n",
    "\n",
    "# new individuals\n",
    "new_X=[]\n",
    "new_Y=[]\n",
    "for j in n_splits:\n",
    "    if j==0:\n",
    "        ind=np.random.choice(np.arange(train_X.shape[0]), size=1)\n",
    "        new_X.append(list(train_X[ind,:,:]))\n",
    "        new_Y.append(list(train_Y[ind,:,:]))\n",
    "    # sample breakpoints uniformly\n",
    "    breaks=np.sort(V.shape[0] * ss.beta.rvs(a=1, b=1, size=j)).astype(int)\n",
    "    # pick founders uniformly at random without replacement and stitch their labels together\n",
    "    founds=np.random.choice(np.arange(train_X.shape[0]), size=j+1, replace=False)\n",
    "    # assemble genome and labels\n",
    "    new_x,new_y = [],[]\n",
    "    new_x.append(train_X[founds[0],:breaks[0],:])\n",
    "    new_y.append(train_Y[founds[0],:breaks[0],:])\n",
    "    for i,found in enumerate(founds[1:-1]):\n",
    "        new_x.append(train_X[found, breaks[i]:breaks[i+1],:])\n",
    "        new_y.append(train_Y[found, breaks[i]:breaks[i+1],:])\n",
    "    new_x.append(train_X[founds[-1], breaks[-1]:,:])\n",
    "    new_y.append(train_Y[founds[-1], breaks[-1]:,:])\n",
    "    new_X.append(np.vstack(new_x))\n",
    "    new_Y.append(np.vstack(new_y))\n",
    "train_X=np.vstack((train_X, new_X))\n",
    "train_Y=np.vstack((train_Y, new_Y))\n",
    "[train_X.shape, train_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(train_Y[-40:,:2048,0].astype(int), aspect='auto', cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_u(input_shape):\n",
    "    \"\"\"\n",
    "    input_shape: The height, width and channels as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    #conv0 = Conv1D(filters=input_shape[1], kernel_size=1, padding='same')(X_input)\n",
    "    #drop1 = Dropout(0.33)(conv0)\n",
    "    ks=8\n",
    "    conv1 = Conv1D(filters=64, kernel_size=ks, padding = 'same')(X_input)\n",
    "    bn1 = BatchNormalization(axis = -1)(conv1)\n",
    "    conv1 = Activation('relu')(bn1)\n",
    "    pool1 = MaxPooling1D(2)(conv1)\n",
    "\n",
    "    # Second convolutional block with maxpool\n",
    "    conv2 = Conv1D(filters=128, kernel_size=ks, padding = 'same', name = 'conv1')(bn1)\n",
    "    bn2 = BatchNormalization(axis = -1)(conv2)\n",
    "    conv2 = Activation('relu')(bn2)\n",
    "    pool2 = MaxPooling1D(2)(conv2)\n",
    "\n",
    "    # Third convolutional block with maxpool\n",
    "    conv3 = Conv1D(filters=512, kernel_size=ks, padding = 'same')(pool2)\n",
    "    bn3 = BatchNormalization(axis = -1)(conv3)\n",
    "    conv3 = Activation('relu')(bn3)    \n",
    "    pool3 = MaxPooling1D(2)(conv3)\n",
    "        \n",
    "    # Now you just go back up\n",
    "    conv4 = Conv1D(filters=512, kernel_size=ks, padding='same')(pool3) # (UpSampling1D(size = 2)(pool3))\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    merge4= concatenate([conv4, pool3], axis=1)\n",
    "    conv5 = Conv1D(filters=512, kernel_size=ks, padding='same')(merge4)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "\n",
    "    conv6 = Conv1D(filters=128, kernel_size=ks, padding = 'same')(conv5) # (UpSampling1D(size = 2)(conv5))\n",
    "    bn6 = BatchNormalization(axis = -1)(conv6)\n",
    "    conv6 = Activation('relu')(bn6)\n",
    "    merge6= concatenate([conv6, pool2], axis=1)\n",
    "    conv7 = Conv1D(filters=64, kernel_size=ks, padding = 'same')(merge6)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    Yhat = Conv1D(k, kernel_size=1, activation = 'softmax')(conv7)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = Yhat, name='model2b')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_shape, ks=4):\n",
    "    # ref: https://github.com/zhixuhao/unet\n",
    "    X_input = Input(shape=input_shape)\n",
    "\n",
    "    fsh=1 # fudge factor to reduce size\n",
    "    conv1 = Conv1D(filters=64, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(X_input)\n",
    "    conv1 = Conv1D(filters=64, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(filters=128//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv1D(filters=128//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv3 = Conv1D(filters=256//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv1D(filters=256//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "    conv4 = Conv1D(filters=512//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv1D(filters=512//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=2)(conv4)\n",
    "        \n",
    "    conv5 = Conv1D(filters=1024//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv1D(filters=1024//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv5)\n",
    "\n",
    "    up6 = Conv1D(filters=512//fsh, kernel_size=ks//2, activation = 'relu', padding = 'same', \n",
    "                 kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv5))\n",
    "    merge6 = concatenate([conv4,up6], axis = -1)\n",
    "    conv6 = Conv1D(filters=512//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv1D(filters=512//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv1D(filters=256//fsh, kernel_size=ks//2, activation = 'relu', padding = 'same', \n",
    "                 kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = -1)\n",
    "    conv7 = Conv1D(filters=256//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv1D(filters=256//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv1D(filters=128//fsh, kernel_size=ks//2, activation = 'relu', padding = 'same', \n",
    "                 kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = -1)\n",
    "    conv8 = Conv1D(filters=128//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv1D(filters=128//fsh, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv1D(filters=64, kernel_size=ks//2, activation = 'relu', padding = 'same', \n",
    "                 kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = -1)\n",
    "    conv9 = Conv1D(filters=64, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv1D(filters=64, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv1D(filters=2*4, kernel_size=ks, activation = 'relu', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv1D(filters=4, kernel_size=1, activation = 'softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = conv10, name='model3')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_shape):\n",
    "    # ref: https://github.com/zhixuhao/unet\n",
    "    X_input = Input(shape=input_shape)\n",
    "    \n",
    "    # hopefully a big feature extractor\n",
    "    '''conv1={}\n",
    "    max_pow=9\n",
    "    for f in [2**i for i in range(1, max_pow)]:\n",
    "        conv1[f] = Conv1D(filters=64, kernel_size=f, activation='tanh', padding='same')(X_input)\n",
    "    conv1 = concatenate(list(conv1.values()), axis=-1)'''\n",
    "    X = Conv1D(filters=64, kernel_size=32, strides=16, padding='same')(X_input)\n",
    "    X = BatchNormalization(axis = -1)(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(filters=64, kernel_size=16, strides=4, activation='tanh', padding='same')(X)\n",
    "    \n",
    "    # do something with those features\n",
    "    rfk=64 # local predictor size\n",
    "    rfn=1 # padding\n",
    "    X = LocallyConnected1D(filters=k, kernel_size=rfk, activation = 'tanh', padding = 'valid', \n",
    "                           strides=rfn, kernel_initializer = 'he_normal')(ZeroPadding1D(padding=(rfk//2,rfk//2 - 1))(X))\n",
    "    \"\"\"conv3 = Conv1D(filters=64, kernel_size=16, activation = 'tanh', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv2)\n",
    "    conv4 = Conv1D(filters=64, kernel_size=16, activation = 'tanh', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv3)\n",
    "    conv5 = Conv1D(filters=64, kernel_size=16, activation = 'tanh', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv4)\n",
    "    conv6 = Conv1D(filters=64, kernel_size=16, activation = 'tanh', padding = 'same', \n",
    "                   kernel_initializer = 'he_normal')(conv5)\"\"\"\n",
    "    X = Conv1D(32, 64, padding='same', activation='tanh')(UpSampling1D(16*4*rfn)(X))\n",
    "    X = Conv1D(8, 8, padding='same', activation='tanh')(X)\n",
    "    X = Conv1D(k, 1, activation='softmax')(X)\n",
    "    #X = Dense(64, activation='tanh')(UpSampling1D(16)(X))\n",
    "    #X = Dense(64, activation='tanh')(X)\n",
    "    \n",
    "    # output layer, sure why not, idk\n",
    "    #conv3 = Conv1D(filters=k, kernel_size=1, activation='softmax', padding='same', \n",
    "    #               kernel_initializer = 'he_normal')(conv2)\n",
    "    # X = Dense(k, activation='softmax')(X)\n",
    "\n",
    "    return Model(inputs=X_input, outputs=X, name='yolo')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "nc,nv= (2048,8192) #train_X.shape[:2]\n",
    "# nv-=4 # as a freaking edge case\n",
    "keep=np.random.choice(np.arange(train_ix.shape[0]), replace=False, size=nc)\n",
    "model = cnn(train_X[keep,:nv].shape[1:])\n",
    "# compile model\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', \n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history=model.fit(train_X[keep,:nv,:], train_Y[keep,:nv,:], \n",
    "                  validation_data=(dev_X[:,:nv,:], dev_Y[:,:nv,:]),\n",
    "                  epochs = 4, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_Y_hat = model.predict(dev_X[:,:nv,:])\n",
    "print(dev_Y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_Y_hat[6,:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=confusion_matrix(dev_Y[:,:nv,0].astype(int).flatten(), \n",
    "                        np.argmax(dev_Y_hat[:,:nv,:], axis=-1).flatten()).T\n",
    "pd.DataFrame(result, index=[k for k,v in sorted(labels.items(), key=lambda x:x[1])], \n",
    "                     columns=[k for k,v in sorted(labels.items(), key=lambda x:x[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on dev set')\n",
    "results = model.evaluate(dev_X[:,:nv,:], dev_Y[:,:nv,:], batch_size=1)\n",
    "print('dev set loss, acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of ancestors as Pois(2.86 * generation_time), over 1-10 (maxgen) generations\n",
    "n_fake=1000\n",
    "maxgen=10\n",
    "n_splits=2+np.hstack([ss.poisson.rvs(2.86*gen, size=n_fake//maxgen) for gen in range(1,maxgen)])\n",
    "\n",
    "# new individuals\n",
    "new_X=[]\n",
    "new_Y=[]\n",
    "for j in n_splits:\n",
    "    if j==0:\n",
    "        ind=np.random.choice(np.arange(dev_X.shape[0]), size=1)\n",
    "        new_X.append(list(dev_X[ind,:,:]))\n",
    "        new_Y.append(list(dev_Y[ind,:]))\n",
    "    # sample breakpoints uniformly\n",
    "    breaks=np.sort(V.shape[0] * ss.beta.rvs(a=1, b=1, size=j)).astype(int)\n",
    "    # pick founders uniformly at random without replacement and stitch their labels together\n",
    "    founds=np.random.choice(np.arange(dev_X.shape[0]), size=j+1, replace=False)\n",
    "    # assemble genome and labels\n",
    "    new_x,new_y = [],[]\n",
    "    new_x.append(dev_X[founds[0],:breaks[0],:])\n",
    "    new_y.append(dev_Y[founds[0],:breaks[0],:])\n",
    "    for i,found in enumerate(founds[1:-1]):\n",
    "        new_x.append(dev_X[found, breaks[i]:breaks[i+1],:])\n",
    "        new_y.append(dev_Y[found, breaks[i]:breaks[i+1],:])\n",
    "    new_x.append(dev_X[founds[-1], breaks[-1]:,:])\n",
    "    new_y.append(dev_Y[founds[-1], breaks[-1]:,:])\n",
    "    new_X.append(np.vstack(new_x))\n",
    "    new_Y.append(np.vstack(new_y))\n",
    "devv_X=np.vstack((dev_X, new_X))\n",
    "devv_Y=np.vstack((dev_Y, new_Y))\n",
    "[devv_X.shape, devv_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on dev set')\n",
    "results = model.evaluate(devv_X[:,:nv,:], devv_Y[:,:nv], batch_size=1)\n",
    "print('dev set loss, acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
