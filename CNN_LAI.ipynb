{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/magu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of cores to 16\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=24, \n",
    "                                                   inter_op_parallelism_threads=24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', (5008,)), ('G', (5008, 57876, 4)), ('V', (57876, 4))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load genetic data\n",
    "chr1kg = np.load('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X.npz')\n",
    "\n",
    "# S are samples, G are genotypes, V are variants\n",
    "[(i,chr1kg[i].shape) for i in chr1kg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HG00096_S1', 'HG00097_S1', 'HG00099_S1', 'HG00100_S1',\n",
       "       'HG00101_S1'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=chr1kg['S'].astype(str)\n",
    "S[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '723307', 'C', 'G'],\n",
       "       ['1', '727841', 'G', 'A']], dtype='<U225')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V=chr1kg['V'].astype(str)\n",
    "V[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=chr1kg['G'].astype(bool)\n",
    "G[:1,:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "chr1kg=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ancestry labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info=pd.read_csv('/home/jsokol/Data/igsr_samples.tsv', sep=\"\\t\")\n",
    "sample_info.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE AMERICANS + ADMIXED POPS\n",
    "pops_to_remove=['ASW','MXL','GIH','ITU','STU','CEU','PUR','PEL','CLM']\n",
    "samples=sample_info[~sample_info['Population code'].isin(pops_to_remove)]\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=[i for i in S if i[:-3] in samples['Sample name'].values]\n",
    "S_pop=[str(samples.loc[samples['Sample name']==i[:-3], \"Superpopulation code\"].values[0]) for i in S]\n",
    "Y=np.array([[pop for _ in range(G.shape[1])] for pop in S_pop])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[np.array([1,345,3012]),:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=samples[\"Superpopulation code\"].dropna().unique().tolist()\n",
    "labels=dict(zip(labels, range(len(labels))))\n",
    "k=len(labels)\n",
    "(k, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2=[]\n",
    "for i in range(Y.shape[0]): # individuals\n",
    "    Y2.append([])\n",
    "    for j in range(Y.shape[1]): # sites\n",
    "        Y2[-1].append(np.zeros(k))\n",
    "        Y2[-1][-1][labels[Y[i,j]]]=1\n",
    "Y=np.array(Y2)\n",
    "Y2=[]\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X_int.train.txt', header=None).iloc[:,0].values\n",
    "dev=pd.read_csv('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X_int.dev.txt', header=None).iloc[:,0].values\n",
    "test=pd.read_csv('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X_int.test.txt', header=None).iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix=np.array([i for i,x in enumerate(S) if x in train])\n",
    "dev_ix=np.array([i for i,x in enumerate(S) if x in dev])\n",
    "test_ix=np.array([i for i,x in enumerate(S) if x in test])\n",
    "[train_ix.shape, dev_ix.shape, test_ix.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=G[train_ix,:,:]\n",
    "train_Y=Y[train_ix,:]\n",
    "[train_X.shape, train_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X=G[dev_ix,:,:]\n",
    "dev_Y=Y[dev_ix,:]\n",
    "[dev_X.shape, dev_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X=G[test_ix,:,:]\n",
    "test_Y=Y[test_ix,:]\n",
    "[test_X.shape, test_Y.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment training set with admixed individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take number of ancestors as Pois(2.86 * generation_time), over 1-10 (maxgen) generations\n",
    "n_fake=10000\n",
    "maxgen=6\n",
    "n_splits=2+np.hstack([ss.poisson.rvs(2.86*gen, size=n_fake//maxgen) for gen in range(1,maxgen)])\n",
    "\n",
    "# new individuals\n",
    "new_X=[]\n",
    "new_Y=[]\n",
    "for j in n_splits:\n",
    "    if j==0:\n",
    "        ind=np.random.choice(np.arange(train_X.shape[0]), size=1)\n",
    "        new_X.append(list(train_X[ind,:,:]))\n",
    "        new_Y.append(list(train_Y[ind,:,:]))\n",
    "    # sample breakpoints uniformly\n",
    "    breaks=np.sort(V.shape[0] * ss.beta.rvs(a=1, b=1, size=j)).astype(int)\n",
    "    # pick founders uniformly at random without replacement and stitch their labels together\n",
    "    founds=np.random.choice(np.arange(train_X.shape[0]), size=j+1, replace=False)\n",
    "    # assemble genome and labels\n",
    "    new_x,new_y = [],[]\n",
    "    new_x.append(train_X[founds[0],:breaks[0],:])\n",
    "    new_y.append(train_Y[founds[0],:breaks[0],:])\n",
    "    for i,found in enumerate(founds[1:-1]):\n",
    "        new_x.append(train_X[found, breaks[i]:breaks[i+1],:])\n",
    "        new_y.append(train_Y[found, breaks[i]:breaks[i+1],:])\n",
    "    new_x.append(train_X[founds[-1], breaks[-1]:,:])\n",
    "    new_y.append(train_Y[founds[-1], breaks[-1]:,:])\n",
    "    new_X.append(np.vstack(new_x))\n",
    "    new_Y.append(np.vstack(new_y))\n",
    "train_X=np.vstack((train_X, new_X))\n",
    "train_Y=np.vstack((train_Y, new_Y))\n",
    "[train_X.shape, train_Y.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(np.dot(train_Y[-1000:,:], np.arange(k)), aspect='auto', cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(input_shape):\n",
    "    \"\"\"\n",
    "    input_shape: The height, width and channels as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "    # ref: https://github.com/zhixuhao/unet\n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(shape=input_shape)\n",
    "\n",
    "    # First convolutional block\n",
    "    #conv0 = Conv1D(filters=input_shape[1], kernel_size=1, padding='same')(X_input)\n",
    "    #drop1 = Dropout(0.33)(conv0)\n",
    "    conv1 = Conv1D(filters=128//2, kernel_size=256//2, padding = 'same')(X_input)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    bn1 = BatchNormalization(axis = -1)(conv1)\n",
    "\n",
    "    # Second convolutional block with maxpool\n",
    "    conv2 = Conv1D(filters=256//2, kernel_size=64//2, padding = 'same', name = 'conv1')(bn1)\n",
    "    bn2 = BatchNormalization(axis = -1)(conv2)\n",
    "    conv2 = Activation('relu')(bn2)\n",
    "    pool2 = MaxPooling1D(2)(conv2)\n",
    "\n",
    "    # Third convolutional block with maxpool\n",
    "    conv3 = Conv1D(filters=64//2, kernel_size=16//2, padding = 'same')(pool2)\n",
    "    bn3 = BatchNormalization(axis = -1)(conv3)\n",
    "    conv3 = Activation('relu')(bn3)    \n",
    "    pool3 = MaxPooling1D(2)(conv3)\n",
    "        \n",
    "    # Now you just go back up\n",
    "    conv4 = Conv1D(filters=256//2, kernel_size=64//2, padding='same')(UpSampling1D(size = 2)(pool3))\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    merge4= conv4 # concatenate([conv4, pool2], axis=1)\n",
    "    conv5 = Conv1D(filters=256//2, kernel_size=64//2, padding='same')(merge4)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "\n",
    "    conv6 = Conv1D(filters=128//2, kernel_size=256//2, padding = 'same')(UpSampling1D(size = 2)(conv5))\n",
    "    bn6 = BatchNormalization(axis = -1)(conv6)\n",
    "    conv6 = Activation('relu')(bn6)\n",
    "    merge6= conv6 # concatenate([conv6, bn1], axis=1)\n",
    "    conv7 = Conv1D(filters=128//2, kernel_size=256//2, padding = 'same')(merge6)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    Yhat = Conv1D(k, kernel_size=1, activation = 'softmax')(conv7)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = Yhat, name='model2b')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "nc,nv=train_X.shape[:2]\n",
    "keep=np.random.choice(np.arange(train_X.shape[0]), replace=False, size=nc)\n",
    "model = model2(train_X[keep,:nv,:].shape[1:])\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_X[keep,:nv,:], train_Y[keep,:nv,:], epochs = 4, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on dev set')\n",
    "results = model.evaluate(dev_X[:,:nv,:], dev_Y[:,:nv,:], batch_size=1)\n",
    "print('dev set loss, acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_lai_v01_20200306.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(train_X[keep,:nv,:], train_Y[keep,:nv,:], verbose=0)\n",
    "_, dev_acc = model.evaluate(dev_X[:,:nv,:], dev_Y[:,:nv,:], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "print('train acc:', train_acc)\n",
    "print('train acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
