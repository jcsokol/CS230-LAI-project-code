{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding1D, BatchNormalization, Flatten, Conv1D\n",
    "from keras.layers import AveragePooling1D, MaxPooling1D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "# from kt_utils import *\n",
    "import keras.backend as K\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy.random import seed\n",
    "seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Matt's 'chr1_ukb_X.npz' file\n",
    "chr1_ukb_X = np.load('/home/magu/deepmix/data/ALL_DNA_dataset/chr1_1kg_X.npz')\n",
    "# create dict mapping from sampleid to population name\n",
    "sampleid_to_population_pandas_df = pd.read_csv('/home/jsokol/Data/igsr_samples_cleaned_version.tsv', delimiter=\"\\t\")\n",
    "sampleid_to_population_dict = sampleid_to_population_pandas_df.set_index('Sample name')['Population code'].to_dict()\n",
    "# remove samples from chr1_ukb_X that are not in dict or that do not contain value\n",
    "temp = set()\n",
    "for x in np.nditer(chr1_ukb_X['S']):\n",
    "    sample_id = str((x.item(0))).replace(\"'\", \"\").replace(\"b\", \"\").split(\"_\")[0]\n",
    "    if sample_id not in sampleid_to_population_dict:\n",
    "        print(sample_id + ' is not in dict. Make sure to remove this sample from the dataset.')\n",
    "    elif sampleid_to_population_dict[sample_id] == float('nan'):\n",
    "        print(sample_id + ' maps to nan in dict')\n",
    "chr1_ukb_X_G = chr1_ukb_X['G'][:,:,:]\n",
    "chr1_ukb_X_V = chr1_ukb_X['V'][:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten input data\n",
    "X_all_data = chr1_ukb_X_G.reshape(chr1_ukb_X_G.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes minus classes deleted because they are admixed = 26\n",
      "{'ESN', 'IBS', 'BEB', 'TSI', 'ASW', 'CHB', 'PJL', 'CHS', 'KHV', 'MXL', 'GBR', 'MSL', 'JPT', 'GIH', 'ACB', 'FIN', 'LWK', 'STU', 'PEL', 'YRI', 'CDX', 'CLM', 'ITU', 'PUR', 'GWD', 'CEU'}\n"
     ]
    }
   ],
   "source": [
    "# create set of all populations represented in dataset\n",
    "populations_represented_set = set()\n",
    "for i in range(5008):\n",
    "    sample_id = str(chr1_ukb_X['S'][i].item(0)).replace(\"'\", \"\").replace(\"b\", \"\").split(\"_\")[0]\n",
    "    populations_represented_set.add(sampleid_to_population_dict[sample_id])\n",
    "# print total number of populations represented by dataset\n",
    "n_classes = len(populations_represented_set)\n",
    "print('Total number of classes minus classes deleted because they are admixed = ' + str(n_classes))\n",
    "print(populations_represented_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode labels and delete unwanted classes\n",
    "Y_all_data = np.zeros((n_classes, 5008))\n",
    "indices_to_delete = list()\n",
    "for i in range(5008):\n",
    "    sample_id = str(chr1_ukb_X['S'][i].item(0)).replace(\"'\", \"\").replace(\"b\", \"\").split(\"_\")[0]\n",
    "    if sampleid_to_population_dict[sample_id] == 'CDX':\n",
    "        Y_all_data[0,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'KHV':\n",
    "        Y_all_data[1,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'CHS':\n",
    "        Y_all_data[2,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'CHB':\n",
    "        Y_all_data[3,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'JPT':\n",
    "        Y_all_data[4,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'PEL':\n",
    "        Y_all_data[5,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'MXL':\n",
    "        Y_all_data[6,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'CLM':\n",
    "        Y_all_data[7,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'PUR':\n",
    "        Y_all_data[8,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'IBS':\n",
    "        Y_all_data[9,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'TSI':\n",
    "        Y_all_data[10,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'CEU':\n",
    "        Y_all_data[11,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'GBR':\n",
    "        Y_all_data[12,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'FIN':\n",
    "        Y_all_data[13,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'GIH':\n",
    "        Y_all_data[14,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'PJL':\n",
    "        Y_all_data[15,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'ITU':\n",
    "        Y_all_data[16,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'STU':\n",
    "        Y_all_data[17,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'BEB':\n",
    "        Y_all_data[18,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'ACB':\n",
    "        Y_all_data[19,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'ASW':\n",
    "        Y_all_data[20,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'LWK':\n",
    "        Y_all_data[21,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'ESN':\n",
    "        Y_all_data[22,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'YRI':\n",
    "        Y_all_data[23,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'MSL':\n",
    "        Y_all_data[24,i] = 1\n",
    "    elif sampleid_to_population_dict[sample_id] == 'GWD':\n",
    "        Y_all_data[25,i] = 1\n",
    "# delete samples with unwanted classes using 'indices_to_delete'\n",
    "X_all_data = np.delete(X_all_data, indices_to_delete, 1)\n",
    "Y_all_data = np.delete(Y_all_data, indices_to_delete, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the order of the data\n",
    "randomize = np.arange(X_all_data.shape[1])\n",
    "np.random.shuffle(randomize)\n",
    "X_all_data = X_all_data[:,randomize]\n",
    "Y_all_data = Y_all_data[:,randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train = X_all_data[:,:int(X_all_data.shape[1]*0.8)]\n",
    "X_test = X_all_data[:,int(X_all_data.shape[1]*0.8):]\n",
    "Y_train = Y_all_data[:,:int(X_all_data.shape[1]*0.8)]\n",
    "Y_test = Y_all_data[:,int(X_all_data.shape[1]*0.8):]\n",
    "# convert to correct format for use in keras\n",
    "X_train = X_train[:,:,np.newaxis]\n",
    "X_train = np.transpose(X_train, (1, 0, 2))\n",
    "X_test = X_test[:,:,np.newaxis]\n",
    "X_test = np.transpose(X_test, (1, 0, 2))\n",
    "Y_train = np.transpose(Y_train, (1, 0))\n",
    "Y_test = np.transpose(Y_test, (1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(n_snps_per_sample):\n",
    "    \"\"\"\n",
    "    input_shape: The height, width and channels as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(shape=(n_snps_per_sample, 1))\n",
    "\n",
    "    # First convolutional block\n",
    "    X = Conv1D(filters=49, kernel_size=500 ,strides=500, padding = 'valid', name = 'conv0')(X_input)\n",
    "    X = BatchNormalization(axis = -1, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second convolutional block\n",
    "    X = Conv1D(49, 75, strides = 5, padding = 'same', name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = -1, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    X = MaxPooling1D(2, name='max_pool')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(n_classes, activation='softmax', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='model2')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min value in confusion matrix = 0.4664595047896507, max value in confusion matrix = 23.80788590396041\n"
     ]
    }
   ],
   "source": [
    "## CREATE CONFUSION MATRIX MANUALLY\n",
    "# 0) settings\n",
    "confusion_matrix_spread = 0.86 # must be between 0 and 1\n",
    "reverse_weights = False\n",
    "# 1) create confusion matrix from 1000 genomes supplementary info handbook's info on relatedness\n",
    "confusion_matrix = np.array([[0.0001,0.0019,0.0049,0.0086,0.0169,0.0864,0.0708,0.0766,0.0826,0.105,0.1051,0.106,0.1062,0.0991,0.073,0.0705,0.0685,0.0671,0.056,0.1405,0.1254,0.1524,0.1631,0.1613,0.1636,0.1581],[0.0019,0.0001,0.0033,0.0062,0.014,0.0823,0.0665,0.0726,0.0788,0.1008,0.1009,0.1016,0.1018,0.095,0.0687,0.0664,0.0643,0.0629,0.0521,0.1379,0.1224,0.1499,0.1608,0.1589,0.1611,0.1557],[0.0049,0.0033,0.0001,0.0011,0.0088,0.082,0.0677,0.0749,0.0816,0.1042,0.1043,0.105,0.1053,0.0977,0.0726,0.0702,0.0682,0.0669,0.0558,0.1418,0.1263,0.1538,0.1647,0.1627,0.1651,0.1595],[0.0086,0.0062,0.0011,0.0001,0.0069,0.0795,0.0652,0.0727,0.0796,0.1022,0.1023,0.1029,0.1032,0.0956,0.0712,0.0687,0.067,0.0657,0.0547,0.1405,0.1248,0.1525,0.1634,0.1614,0.1638,0.1582],[0.0169,0.014,0.0088,0.0069,0.0001,0.0798,0.0659,0.0738,0.0809,0.1039,0.1041,0.1046,0.1049,0.0971,0.0724,0.0701,0.0683,0.0668,0.0564,0.1419,0.1262,0.1538,0.1648,0.1628,0.1652,0.1595],[0.0864,0.0823,0.082,0.0795,0.0798,0.0001,0.0174,0.0391,0.0564,0.0856,0.0856,0.0844,0.0848,0.0806,0.074,0.0705,0.0742,0.0739,0.068,0.1377,0.119,0.1512,0.1619,0.1599,0.162,0.1517],[0.0708,0.0665,0.0677,0.0652,0.0659,0.0174,0.0001,0.0093,0.0189,0.0359,0.0367,0.0358,0.036,0.0356,0.038,0.0337,0.0398,0.0398,0.0353,0.1033,0.0829,0.1178,0.1289,0.1272,0.1282,0.1238],[0.0766,0.0726,0.0749,0.0727,0.0738,0.0391,0.0093,0.0001,0.0056,0.014,0.0152,0.0152,0.0154,0.0184,0.0281,0.0234,0.0311,0.0312,0.0284,0.0879,0.0677,0.103,0.114,0.1125,0.1135,0.1091],[0.0826,0.0788,0.0816,0.0796,0.0809,0.0564,0.0189,0.0056,0.0001,0.0087,0.0097,0.0108,0.0109,0.0155,0.0275,0.0226,0.0309,0.0311,0.0292,0.0756,0.0566,0.0905,0.101,0.0995,0.1004,0.0962],[0.105,0.1008,0.1042,0.1022,0.1039,0.0856,0.0359,0.014,0.0087,0.0001,0.0016,0.0024,0.0024,0.0103,0.0343,0.0286,0.0395,0.0401,0.0393,0.1065,0.0861,0.1232,0.1354,0.1336,0.1352,0.1295],[0.1051,0.1009,0.1043,0.1023,0.1041,0.0865,0.0367,0.0152,0.0097,0.0016,0.0001,0.0036,0.0038,0.0118,0.0329,0.0273,0.0381,0.0388,0.0382,0.1078,0.0875,0.1244,0.1367,0.1349,0.1365,0.1308],[0.106,0.1016,0.105,0.1029,0.1046,0.0844,0.0358,0.0152,0.0108,0.0024,0.0036,0.0001,0.0003,0.0064,0.0338,0.0281,0.0396,0.0401,0.039,0.1107,0.0899,0.1278,0.14,0.1383,0.1398,0.1342],[0.1062,0.1018,0.1053,0.1032,0.1049,0.0848,0.036,0.0154,0.0109,0.0024,0.0038,0.0003,0.0001,0.0068,0.0341,0.0284,0.0399,0.0405,0.0393,0.1106,0.0899,0.1276,0.1398,0.1381,0.1396,0.1341],[0.0991,0.095,0.0977,0.0956,0.0971,0.0806,0.0356,0.0184,0.0155,0.0103,0.0118,0.0064,0.0068,0.0001,0.035,0.0296,0.0402,0.0406,0.0386,0.1135,0.0928,0.1304,0.1425,0.1407,0.1423,0.1368],[0.073,0.0687,0.0726,0.0712,0.0724,0.074,0.038,0.0281,0.0275,0.0343,0.0329,0.0338,0.0341,0.035,0.0001,0.0037,0.0039,0.0044,0.0045,0.1028,0.0841,0.1169,0.1287,0.1271,0.1286,0.1236],[0.0705,0.0664,0.0702,0.0687,0.0701,0.0705,0.0337,0.0234,0.0226,0.0286,0.0273,0.0281,0.0284,0.0296,0.0037,0.0001,0.0033,0.0037,0.0038,0.0991,0.0802,0.1135,0.1251,0.1237,0.1249,0.1201],[0.0685,0.0643,0.0682,0.067,0.0683,0.0742,0.0398,0.0311,0.0309,0.0395,0.0381,0.0396,0.0399,0.0402,0.0039,0.0033,0.0001,0.0012,0.0024,0.1021,0.0839,0.1158,0.1274,0.1259,0.1272,0.1224],[0.0671,0.0629,0.0669,0.0657,0.0668,0.0739,0.0398,0.0312,0.0311,0.0401,0.0388,0.0401,0.0405,0.0406,0.0044,0.0037,0.0012,0.0001,0.0022,0.1016,0.0835,0.1152,0.1268,0.1252,0.1265,0.1217],[0.056,0.0521,0.0558,0.0547,0.0564,0.068,0.0353,0.0284,0.0292,0.0393,0.0382,0.039,0.0393,0.0386,0.0045,0.0038,0.0024,0.0022,0.0001,0.1001,0.0819,0.1138,0.1252,0.1237,0.1249,0.1204],[0.1405,0.1379,0.1418,0.1405,0.1419,0.1377,0.1033,0.0879,0.0756,0.1065,0.1078,0.1107,0.1106,0.1135,0.1028,0.0991,0.1021,0.1016,0.1001,0.0001,0.0026,0.0065,0.0036,0.0027,0.0046,0.0061],[0.1254,0.1224,0.1263,0.1248,0.1262,0.119,0.0829,0.0677,0.0566,0.0861,0.0875,0.0899,0.0899,0.0928,0.0841,0.0802,0.0839,0.0835,0.0819,0.0026,0.0001,0.0097,0.0098,0.0089,0.0103,0.0107],[0.1524,0.1499,0.1538,0.1525,0.1538,0.1512,0.1178,0.103,0.0905,0.1232,0.1244,0.1278,0.1276,0.1304,0.1169,0.1135,0.1158,0.1152,0.1138,0.0065,0.0097,0.0001,0.008,0.0073,0.0096,0.0109],[0.1631,0.1608,0.1647,0.1634,0.1648,0.1619,0.1289,0.114,0.101,0.1354,0.1367,0.14,0.1398,0.1425,0.1287,0.1251,0.1274,0.1268,0.1252,0.0036,0.0098,0.008,0.0001,0.0009,0.0053,0.0075],[0.1613,0.1589,0.1627,0.1614,0.1628,0.1599,0.1272,0.1125,0.0995,0.1336,0.1349,0.1383,0.1381,0.1407,0.1271,0.1237,0.1259,0.1252,0.1237,0.0027,0.0089,0.0073,0.0009,0.0001,0.004,0.0062],[0.1636,0.1611,0.1651,0.1638,0.1652,0.162,0.1282,0.1135,0.1004,0.1352,0.1365,0.1398,0.1396,0.1423,0.1286,0.1249,0.1272,0.1265,0.1249,0.0046,0.0103,0.0096,0.0053,0.004,0.0001,0.0037],[0.1581,0.1557,0.1595,0.1582,0.1595,0.1567,0.1238,0.1091,0.0962,0.1295,0.1308,0.1342,0.1341,0.1368,0.1236,0.1202,0.1224,0.1217,0.1204,0.0061,0.0107,0.0109,0.0075,0.0062,0.0037,0.0001]])\n",
    "# 2) set values along axis to mean values\n",
    "mean = np.mean(confusion_matrix)\n",
    "std = np.std(confusion_matrix)\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    confusion_matrix[i,i] = mean\n",
    "# 3) normalize confusion matrix\n",
    "confusion_matrix = (+confusion_matrix-mean)/(std**confusion_matrix_spread) + 1\n",
    "# 4) take reciprocal since more related populations should have relatively larger coefficients\n",
    "confusion_matrix = 1/confusion_matrix\n",
    "# 5) make sure that the minimum value is 1\n",
    "# confusion_matrix = confusion_matrix + (1 - np.min(confusion_matrix))\n",
    "# 6) reverse weights \n",
    "if reverse_weights == True:\n",
    "    confusion_matrix = -(confusion_matrix - 1) + 1\n",
    "# 7) convert to tensor\n",
    "print('min value in confusion matrix = ' + str(np.min(confusion_matrix)) + ', max value in confusion matrix = ' + str(np.max(confusion_matrix)))\n",
    "confusion_matrix = K.constant(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom loss function \n",
    "def loss_fn(y, yhat): # make sure to use keras.backend (imported as K) and not numpy for all functions (i.e. the log function)\n",
    "    # get predictions and stack these so we have a tensor of [[predicted_i, actual_i], ...,] for each i in batch\n",
    "    beta = 1e10\n",
    "    x_range = tf.range(n_classes, dtype=y.dtype)\n",
    "    y_idx = tf.reduce_sum(tf.nn.softmax(y*beta)*x_range, axis=-1)\n",
    "    y_idx = K.cast(y_idx, dtype='int32')\n",
    "    x_range = tf.range(n_classes, dtype=yhat.dtype)\n",
    "    yhat_idx = tf.reduce_sum(tf.nn.softmax(yhat*beta)*x_range, axis=-1)\n",
    "    yhat_idx = K.cast(yhat_idx, dtype='int32')\n",
    "    indices = K.stack([yhat_idx, y_idx], axis=-1)\n",
    "    # use tf.gather_nd() to convert indices to the appropriate weight from our matrix [w_i, ...] for each i in batch\n",
    "    loss_coefficients_vector = tf.gather_nd(confusion_matrix, indices)  \n",
    "    \n",
    "    # print intermediate variables for debugging\n",
    "  #  print_op = tf.print(loss_coefficients_vector, y, yhat, loss_coefficients_vector*K.categorical_crossentropy(y, yhat), sep=',', summarize = -1)\n",
    "    \n",
    "    # return the sum of the penalties\n",
    "#    with tf.control_dependencies([print_op]):\n",
    "#        return loss_coefficients_vector*K.categorical_crossentropy(y, yhat)\n",
    "    return loss_coefficients_vector*K.categorical_crossentropy(y, yhat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 231504, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv1D)               (None, 463, 49)           24549     \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 463, 49)           196       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 463, 49)           0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 93, 49)            180124    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 93, 49)            196       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 93, 49)            0         \n",
      "_________________________________________________________________\n",
      "max_pool (MaxPooling1D)      (None, 46, 49)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2254)              0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 26)                58630     \n",
      "=================================================================\n",
      "Total params: 263,695\n",
      "Trainable params: 263,499\n",
      "Non-trainable params: 196\n",
      "_________________________________________________________________\n",
      "Learning rate = 8e-05\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model2 = model2(X_train.shape[1])\n",
    "# compile model\n",
    "adam_optimizer_fn = optimizers.Adam(lr=0.00008)\n",
    "model2.compile(optimizer=adam_optimizer_fn, loss=loss_fn, metrics=['accuracy']) \n",
    "# summarize model\n",
    "model2.summary()\n",
    "print('Learning rate = ' + str(K.eval(model2.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4006 samples, validate on 1002 samples\n",
      "Epoch 1/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 13.0997 - accuracy: 0.0991 - val_loss: 10.5004 - val_accuracy: 0.0429\n",
      "Epoch 2/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 14.1750 - accuracy: 0.3590 - val_loss: 12.6599 - val_accuracy: 0.0888\n",
      "Epoch 3/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.6537 - accuracy: 0.6176 - val_loss: 17.7843 - val_accuracy: 0.1986\n",
      "Epoch 4/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 4.1008 - accuracy: 0.7816 - val_loss: 17.2392 - val_accuracy: 0.3134\n",
      "Epoch 5/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 2.4871 - accuracy: 0.8794 - val_loss: 15.8310 - val_accuracy: 0.3413\n",
      "Epoch 6/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.5098 - accuracy: 0.9383 - val_loss: 15.5648 - val_accuracy: 0.3553\n",
      "Epoch 7/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.0606 - accuracy: 0.9708 - val_loss: 15.3815 - val_accuracy: 0.3723\n",
      "Epoch 8/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.8488 - accuracy: 0.9858 - val_loss: 15.3542 - val_accuracy: 0.3723\n",
      "Epoch 9/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.7666 - accuracy: 0.9908 - val_loss: 15.2973 - val_accuracy: 0.3872\n",
      "Epoch 10/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.6961 - accuracy: 0.9940 - val_loss: 14.9452 - val_accuracy: 0.3982\n",
      "Epoch 11/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.6349 - accuracy: 0.9953 - val_loss: 14.9448 - val_accuracy: 0.4092\n",
      "Epoch 12/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.5735 - accuracy: 0.9973 - val_loss: 14.6823 - val_accuracy: 0.4112\n",
      "Epoch 13/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.5304 - accuracy: 0.9973 - val_loss: 14.7785 - val_accuracy: 0.4062\n",
      "Epoch 14/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.4877 - accuracy: 0.9980 - val_loss: 14.4362 - val_accuracy: 0.4301\n",
      "Epoch 15/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.4465 - accuracy: 0.9983 - val_loss: 14.4523 - val_accuracy: 0.4212\n",
      "Epoch 16/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 0.4102 - accuracy: 0.9988 - val_loss: 14.3443 - val_accuracy: 0.4341\n",
      "Epoch 17/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.3771 - accuracy: 0.9988 - val_loss: 14.2727 - val_accuracy: 0.4351\n",
      "Epoch 18/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.3432 - accuracy: 0.9988 - val_loss: 14.0767 - val_accuracy: 0.4331\n",
      "Epoch 19/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.3129 - accuracy: 0.9990 - val_loss: 13.7487 - val_accuracy: 0.4541\n",
      "Epoch 20/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.2840 - accuracy: 0.9990 - val_loss: 13.5445 - val_accuracy: 0.4561\n",
      "Epoch 21/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.2598 - accuracy: 0.9990 - val_loss: 13.7107 - val_accuracy: 0.4551\n",
      "Epoch 22/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.2356 - accuracy: 0.9995 - val_loss: 13.8722 - val_accuracy: 0.4541\n",
      "Epoch 23/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.2146 - accuracy: 0.9995 - val_loss: 13.7790 - val_accuracy: 0.4611\n",
      "Epoch 24/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1952 - accuracy: 0.9998 - val_loss: 13.7872 - val_accuracy: 0.4551\n",
      "Epoch 25/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1775 - accuracy: 1.0000 - val_loss: 13.6541 - val_accuracy: 0.4721\n",
      "Epoch 26/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1613 - accuracy: 1.0000 - val_loss: 13.7647 - val_accuracy: 0.4760\n",
      "Epoch 27/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1470 - accuracy: 1.0000 - val_loss: 13.6341 - val_accuracy: 0.4731\n",
      "Epoch 28/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1340 - accuracy: 1.0000 - val_loss: 13.3192 - val_accuracy: 0.4830\n",
      "Epoch 29/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 0.1222 - accuracy: 1.0000 - val_loss: 13.4712 - val_accuracy: 0.4830\n",
      "Epoch 30/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1115 - accuracy: 1.0000 - val_loss: 13.2624 - val_accuracy: 0.4850\n",
      "Epoch 31/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.1021 - accuracy: 1.0000 - val_loss: 13.1493 - val_accuracy: 0.4940\n",
      "Epoch 32/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0931 - accuracy: 1.0000 - val_loss: 12.8650 - val_accuracy: 0.5020\n",
      "Epoch 33/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0849 - accuracy: 1.0000 - val_loss: 13.4214 - val_accuracy: 0.4920\n",
      "Epoch 34/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0782 - accuracy: 1.0000 - val_loss: 12.9142 - val_accuracy: 0.5080\n",
      "Epoch 35/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0715 - accuracy: 1.0000 - val_loss: 13.1392 - val_accuracy: 0.4960\n",
      "Epoch 36/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 12.7948 - val_accuracy: 0.5150\n",
      "Epoch 37/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0602 - accuracy: 1.0000 - val_loss: 13.1052 - val_accuracy: 0.5090\n",
      "Epoch 38/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 12.7155 - val_accuracy: 0.5250\n",
      "Epoch 39/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 13.0982 - val_accuracy: 0.5120\n",
      "Epoch 40/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 12.9188 - val_accuracy: 0.5220\n",
      "Epoch 41/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0430 - accuracy: 1.0000 - val_loss: 12.9525 - val_accuracy: 0.5269\n",
      "Epoch 42/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 12.6368 - val_accuracy: 0.5329\n",
      "Epoch 43/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 12.9900 - val_accuracy: 0.5299\n",
      "Epoch 44/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 12.7609 - val_accuracy: 0.5269\n",
      "Epoch 45/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 12.7893 - val_accuracy: 0.5309\n",
      "Epoch 46/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 12.8837 - val_accuracy: 0.5399\n",
      "Epoch 47/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 12.6882 - val_accuracy: 0.5379\n",
      "Epoch 48/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 12.5831 - val_accuracy: 0.5439\n",
      "Epoch 49/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 12.8668 - val_accuracy: 0.5459\n",
      "Epoch 50/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 12.6727 - val_accuracy: 0.5419\n",
      "Epoch 51/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 12.5344 - val_accuracy: 0.5489\n",
      "Epoch 52/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 12.4478 - val_accuracy: 0.5479\n",
      "Epoch 53/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 12.5789 - val_accuracy: 0.5579\n",
      "Epoch 54/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 12.5772 - val_accuracy: 0.5519\n",
      "Epoch 55/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 12.5745 - val_accuracy: 0.5549\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 12.4415 - val_accuracy: 0.5519\n",
      "Epoch 57/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 12.5840 - val_accuracy: 0.5539\n",
      "Epoch 58/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 12.5759 - val_accuracy: 0.5539\n",
      "Epoch 59/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 12.6366 - val_accuracy: 0.5579\n",
      "Epoch 60/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 12.7087 - val_accuracy: 0.5539\n",
      "Epoch 61/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 12.6863 - val_accuracy: 0.5629\n",
      "Epoch 62/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 12.6201 - val_accuracy: 0.5559\n",
      "Epoch 63/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 12.5047 - val_accuracy: 0.5619\n",
      "Epoch 64/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 12.7098 - val_accuracy: 0.5589\n",
      "Epoch 65/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 12.7846 - val_accuracy: 0.5629\n",
      "Epoch 66/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 12.3967 - val_accuracy: 0.5679\n",
      "Epoch 67/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 12.4434 - val_accuracy: 0.5709\n",
      "Epoch 68/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 12.5016 - val_accuracy: 0.5669\n",
      "Epoch 69/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 12.5897 - val_accuracy: 0.5729\n",
      "Epoch 70/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 12.6448 - val_accuracy: 0.5709\n",
      "Epoch 71/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 12.5806 - val_accuracy: 0.5768\n",
      "Epoch 72/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 12.4727 - val_accuracy: 0.5768\n",
      "Epoch 73/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 12.6111 - val_accuracy: 0.5739\n",
      "Epoch 74/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 12.6240 - val_accuracy: 0.5719\n",
      "Epoch 75/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 12.4924 - val_accuracy: 0.5758\n",
      "Epoch 76/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 12.5693 - val_accuracy: 0.5768\n",
      "Epoch 77/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 12.5609 - val_accuracy: 0.5828\n",
      "Epoch 78/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 12.6963 - val_accuracy: 0.5818\n",
      "Epoch 79/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 12.7494 - val_accuracy: 0.5778\n",
      "Epoch 80/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 12.4588 - val_accuracy: 0.5838\n",
      "Epoch 81/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 12.4907 - val_accuracy: 0.5838\n",
      "Epoch 82/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 12.5422 - val_accuracy: 0.5828\n",
      "Epoch 83/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 12.5070 - val_accuracy: 0.5818\n",
      "Epoch 84/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 12.5854 - val_accuracy: 0.5878\n",
      "Epoch 85/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 12.6847 - val_accuracy: 0.5798\n",
      "Epoch 86/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 12.5958 - val_accuracy: 0.5908\n",
      "Epoch 87/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 12.6636 - val_accuracy: 0.5868\n",
      "Epoch 88/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 12.7309 - val_accuracy: 0.5878\n",
      "Epoch 89/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 12.6879 - val_accuracy: 0.5888\n",
      "Epoch 90/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 12.6086 - val_accuracy: 0.5918\n",
      "Epoch 91/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 12.7340 - val_accuracy: 0.5878\n",
      "Epoch 92/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 12.8356 - val_accuracy: 0.5848\n",
      "Epoch 93/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 9.4487e-04 - accuracy: 1.0000 - val_loss: 12.8016 - val_accuracy: 0.5898\n",
      "Epoch 94/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 8.8362e-04 - accuracy: 1.0000 - val_loss: 12.8575 - val_accuracy: 0.5808\n",
      "Epoch 95/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 8.2438e-04 - accuracy: 1.0000 - val_loss: 12.9597 - val_accuracy: 0.5888\n",
      "Epoch 96/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.6841e-04 - accuracy: 1.0000 - val_loss: 13.0112 - val_accuracy: 0.5878\n",
      "Epoch 97/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.1640e-04 - accuracy: 1.0000 - val_loss: 12.6453 - val_accuracy: 0.5968\n",
      "Epoch 98/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.7205e-04 - accuracy: 1.0000 - val_loss: 12.8353 - val_accuracy: 0.5898\n",
      "Epoch 99/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.2656e-04 - accuracy: 1.0000 - val_loss: 12.8241 - val_accuracy: 0.5978\n",
      "Epoch 100/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 5.8430e-04 - accuracy: 1.0000 - val_loss: 12.7610 - val_accuracy: 0.5958\n",
      "Epoch 101/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 5.4695e-04 - accuracy: 1.0000 - val_loss: 12.8829 - val_accuracy: 0.5938\n",
      "Epoch 102/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 5.0913e-04 - accuracy: 1.0000 - val_loss: 13.0069 - val_accuracy: 0.5928\n",
      "Epoch 103/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 4.7438e-04 - accuracy: 1.0000 - val_loss: 13.0189 - val_accuracy: 0.5958\n",
      "Epoch 104/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 4.4266e-04 - accuracy: 1.0000 - val_loss: 13.0355 - val_accuracy: 0.5908\n",
      "Epoch 105/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 4.1291e-04 - accuracy: 1.0000 - val_loss: 12.8777 - val_accuracy: 0.5978\n",
      "Epoch 106/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 3.8643e-04 - accuracy: 1.0000 - val_loss: 13.2046 - val_accuracy: 0.5858\n",
      "Epoch 107/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 3.6043e-04 - accuracy: 1.0000 - val_loss: 13.0955 - val_accuracy: 0.5958\n",
      "Epoch 108/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 3.3616e-04 - accuracy: 1.0000 - val_loss: 12.9763 - val_accuracy: 0.5898\n",
      "Epoch 109/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 3.1454e-04 - accuracy: 1.0000 - val_loss: 13.1247 - val_accuracy: 0.5888\n",
      "Epoch 110/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 2.9322e-04 - accuracy: 1.0000 - val_loss: 13.2777 - val_accuracy: 0.5908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 2.7430e-04 - accuracy: 1.0000 - val_loss: 13.1351 - val_accuracy: 0.5898\n",
      "Epoch 112/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 2.5498e-04 - accuracy: 1.0000 - val_loss: 13.3251 - val_accuracy: 0.5918\n",
      "Epoch 113/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 2.3794e-04 - accuracy: 1.0000 - val_loss: 13.1327 - val_accuracy: 0.5958\n",
      "Epoch 114/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 2.2274e-04 - accuracy: 1.0000 - val_loss: 13.4926 - val_accuracy: 0.5928\n",
      "Epoch 115/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 2.0752e-04 - accuracy: 1.0000 - val_loss: 13.1698 - val_accuracy: 0.5948\n",
      "Epoch 116/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.9401e-04 - accuracy: 1.0000 - val_loss: 13.4018 - val_accuracy: 0.5948\n",
      "Epoch 117/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.8090e-04 - accuracy: 1.0000 - val_loss: 13.1559 - val_accuracy: 0.5968\n",
      "Epoch 118/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.6886e-04 - accuracy: 1.0000 - val_loss: 13.3878 - val_accuracy: 0.5898\n",
      "Epoch 119/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.5772e-04 - accuracy: 1.0000 - val_loss: 13.4195 - val_accuracy: 0.5998\n",
      "Epoch 120/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.4776e-04 - accuracy: 1.0000 - val_loss: 13.2765 - val_accuracy: 0.5968\n",
      "Epoch 121/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 1.3753e-04 - accuracy: 1.0000 - val_loss: 13.3087 - val_accuracy: 0.5958\n",
      "Epoch 122/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.2880e-04 - accuracy: 1.0000 - val_loss: 13.3533 - val_accuracy: 0.6008\n",
      "Epoch 123/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.2009e-04 - accuracy: 1.0000 - val_loss: 13.3951 - val_accuracy: 0.6008\n",
      "Epoch 124/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.1213e-04 - accuracy: 1.0000 - val_loss: 13.3291 - val_accuracy: 0.5998\n",
      "Epoch 125/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.0513e-04 - accuracy: 1.0000 - val_loss: 13.4488 - val_accuracy: 0.5978\n",
      "Epoch 126/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 9.7956e-05 - accuracy: 1.0000 - val_loss: 13.5793 - val_accuracy: 0.5968\n",
      "Epoch 127/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 9.1933e-05 - accuracy: 1.0000 - val_loss: 13.4341 - val_accuracy: 0.6038\n",
      "Epoch 128/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 8.5397e-05 - accuracy: 1.0000 - val_loss: 13.6858 - val_accuracy: 0.5978\n",
      "Epoch 129/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 7.9816e-05 - accuracy: 1.0000 - val_loss: 13.6918 - val_accuracy: 0.6008\n",
      "Epoch 130/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 7.4881e-05 - accuracy: 1.0000 - val_loss: 13.5527 - val_accuracy: 0.5958\n",
      "Epoch 131/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.9477e-05 - accuracy: 1.0000 - val_loss: 13.7781 - val_accuracy: 0.5928\n",
      "Epoch 132/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 6.5153e-05 - accuracy: 1.0000 - val_loss: 13.7089 - val_accuracy: 0.6018\n",
      "Epoch 133/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1045e-05 - accuracy: 1.0000 - val_loss: 13.5204 - val_accuracy: 0.5968\n",
      "Epoch 134/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 5.6848e-05 - accuracy: 1.0000 - val_loss: 13.5341 - val_accuracy: 0.6048\n",
      "Epoch 135/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 5.3013e-05 - accuracy: 1.0000 - val_loss: 13.6629 - val_accuracy: 0.5978\n",
      "Epoch 136/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 4.9650e-05 - accuracy: 1.0000 - val_loss: 13.6297 - val_accuracy: 0.6048\n",
      "Epoch 137/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 4.6374e-05 - accuracy: 1.0000 - val_loss: 13.5777 - val_accuracy: 0.6048\n",
      "Epoch 138/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 4.3410e-05 - accuracy: 1.0000 - val_loss: 13.8797 - val_accuracy: 0.6008\n",
      "Epoch 139/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 4.0474e-05 - accuracy: 1.0000 - val_loss: 13.6485 - val_accuracy: 0.6058\n",
      "Epoch 140/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 3.7808e-05 - accuracy: 1.0000 - val_loss: 13.7752 - val_accuracy: 0.5978\n",
      "Epoch 141/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 3.5497e-05 - accuracy: 1.0000 - val_loss: 13.8534 - val_accuracy: 0.5968\n",
      "Epoch 142/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 3.3064e-05 - accuracy: 1.0000 - val_loss: 13.8705 - val_accuracy: 0.6038\n",
      "Epoch 143/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 3.0842e-05 - accuracy: 1.0000 - val_loss: 14.0782 - val_accuracy: 0.6038\n",
      "Epoch 144/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 2.8797e-05 - accuracy: 1.0000 - val_loss: 13.9469 - val_accuracy: 0.6028\n",
      "Epoch 145/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 2.7079e-05 - accuracy: 1.0000 - val_loss: 13.9816 - val_accuracy: 0.5988\n",
      "Epoch 146/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 2.5232e-05 - accuracy: 1.0000 - val_loss: 13.9391 - val_accuracy: 0.6048\n",
      "Epoch 147/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 2.3567e-05 - accuracy: 1.0000 - val_loss: 13.8572 - val_accuracy: 0.6018\n",
      "Epoch 148/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 2.2176e-05 - accuracy: 1.0000 - val_loss: 14.0650 - val_accuracy: 0.5998\n",
      "Epoch 149/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 2.0608e-05 - accuracy: 1.0000 - val_loss: 13.9638 - val_accuracy: 0.6058\n",
      "Epoch 150/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.9246e-05 - accuracy: 1.0000 - val_loss: 13.9859 - val_accuracy: 0.6068\n",
      "Epoch 151/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.7958e-05 - accuracy: 1.0000 - val_loss: 13.9816 - val_accuracy: 0.6128\n",
      "Epoch 152/300\n",
      "4006/4006 [==============================] - 12s 3ms/step - loss: 1.6830e-05 - accuracy: 1.0000 - val_loss: 14.1561 - val_accuracy: 0.5998\n",
      "Epoch 153/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.5697e-05 - accuracy: 1.0000 - val_loss: 14.1825 - val_accuracy: 0.6058\n",
      "Epoch 154/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.4776e-05 - accuracy: 1.0000 - val_loss: 13.9904 - val_accuracy: 0.6118\n",
      "Epoch 155/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.3782e-05 - accuracy: 1.0000 - val_loss: 14.0353 - val_accuracy: 0.6078\n",
      "Epoch 156/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.2853e-05 - accuracy: 1.0000 - val_loss: 14.2597 - val_accuracy: 0.6048\n",
      "Epoch 157/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.2065e-05 - accuracy: 1.0000 - val_loss: 14.1649 - val_accuracy: 0.6108\n",
      "Epoch 158/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 1.1242e-05 - accuracy: 1.0000 - val_loss: 14.3129 - val_accuracy: 0.6078\n",
      "Epoch 159/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.0510e-05 - accuracy: 1.0000 - val_loss: 14.1592 - val_accuracy: 0.6078\n",
      "Epoch 160/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 9.8094e-06 - accuracy: 1.0000 - val_loss: 14.3924 - val_accuracy: 0.6078\n",
      "Epoch 161/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 9.2571e-06 - accuracy: 1.0000 - val_loss: 14.3960 - val_accuracy: 0.6118\n",
      "Epoch 162/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 8.6141e-06 - accuracy: 1.0000 - val_loss: 14.3323 - val_accuracy: 0.6058\n",
      "Epoch 163/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 8.0662e-06 - accuracy: 1.0000 - val_loss: 14.3186 - val_accuracy: 0.6148\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4006/4006 [==============================] - 9s 2ms/step - loss: 7.5447e-06 - accuracy: 1.0000 - val_loss: 14.4368 - val_accuracy: 0.6088\n",
      "Epoch 165/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 7.0466e-06 - accuracy: 1.0000 - val_loss: 14.5811 - val_accuracy: 0.6178\n",
      "Epoch 166/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 6.5853e-06 - accuracy: 1.0000 - val_loss: 14.4907 - val_accuracy: 0.6148\n",
      "Epoch 167/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 6.1616e-06 - accuracy: 1.0000 - val_loss: 14.4835 - val_accuracy: 0.6128\n",
      "Epoch 168/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 5.7487e-06 - accuracy: 1.0000 - val_loss: 14.6022 - val_accuracy: 0.6108\n",
      "Epoch 169/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 5.3951e-06 - accuracy: 1.0000 - val_loss: 14.5500 - val_accuracy: 0.6138\n",
      "Epoch 170/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 5.0658e-06 - accuracy: 1.0000 - val_loss: 14.5259 - val_accuracy: 0.6118\n",
      "Epoch 171/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 4.7366e-06 - accuracy: 1.0000 - val_loss: 14.6225 - val_accuracy: 0.6178\n",
      "Epoch 172/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 4.4194e-06 - accuracy: 1.0000 - val_loss: 14.5967 - val_accuracy: 0.6128\n",
      "Epoch 173/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 4.1458e-06 - accuracy: 1.0000 - val_loss: 14.6477 - val_accuracy: 0.6168\n",
      "Epoch 174/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 3.8883e-06 - accuracy: 1.0000 - val_loss: 14.7044 - val_accuracy: 0.6158\n",
      "Epoch 175/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 3.6232e-06 - accuracy: 1.0000 - val_loss: 14.6347 - val_accuracy: 0.6188\n",
      "Epoch 176/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 3.3993e-06 - accuracy: 1.0000 - val_loss: 14.6793 - val_accuracy: 0.6148\n",
      "Epoch 177/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 3.1725e-06 - accuracy: 1.0000 - val_loss: 14.6989 - val_accuracy: 0.6208\n",
      "Epoch 178/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.9605e-06 - accuracy: 1.0000 - val_loss: 14.9142 - val_accuracy: 0.6098\n",
      "Epoch 179/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.7908e-06 - accuracy: 1.0000 - val_loss: 15.0125 - val_accuracy: 0.6158\n",
      "Epoch 180/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.6050e-06 - accuracy: 1.0000 - val_loss: 14.8024 - val_accuracy: 0.6168\n",
      "Epoch 181/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.4419e-06 - accuracy: 1.0000 - val_loss: 14.9329 - val_accuracy: 0.6148\n",
      "Epoch 182/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.2775e-06 - accuracy: 1.0000 - val_loss: 14.8339 - val_accuracy: 0.6168\n",
      "Epoch 183/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.1378e-06 - accuracy: 1.0000 - val_loss: 15.1962 - val_accuracy: 0.6118\n",
      "Epoch 184/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.9974e-06 - accuracy: 1.0000 - val_loss: 14.8954 - val_accuracy: 0.6168\n",
      "Epoch 185/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.8742e-06 - accuracy: 1.0000 - val_loss: 15.1161 - val_accuracy: 0.6128\n",
      "Epoch 186/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.7617e-06 - accuracy: 1.0000 - val_loss: 15.0959 - val_accuracy: 0.6198\n",
      "Epoch 187/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.6441e-06 - accuracy: 1.0000 - val_loss: 15.1111 - val_accuracy: 0.6188\n",
      "Epoch 188/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.5405e-06 - accuracy: 1.0000 - val_loss: 15.1425 - val_accuracy: 0.6178\n",
      "Epoch 189/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.4438e-06 - accuracy: 1.0000 - val_loss: 15.1319 - val_accuracy: 0.6178\n",
      "Epoch 190/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.3591e-06 - accuracy: 1.0000 - val_loss: 15.1048 - val_accuracy: 0.6218\n",
      "Epoch 191/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 1.2740e-06 - accuracy: 1.0000 - val_loss: 15.2985 - val_accuracy: 0.6138\n",
      "Epoch 192/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.1915e-06 - accuracy: 1.0000 - val_loss: 15.4478 - val_accuracy: 0.6168\n",
      "Epoch 193/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 1.1197e-06 - accuracy: 1.0000 - val_loss: 15.5113 - val_accuracy: 0.6138\n",
      "Epoch 194/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.0536e-06 - accuracy: 1.0000 - val_loss: 15.3799 - val_accuracy: 0.6168\n",
      "Epoch 195/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 9.8382e-07 - accuracy: 1.0000 - val_loss: 15.4713 - val_accuracy: 0.6088\n",
      "Epoch 196/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 9.2020e-07 - accuracy: 1.0000 - val_loss: 15.5639 - val_accuracy: 0.6108\n",
      "Epoch 197/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 8.6753e-07 - accuracy: 1.0000 - val_loss: 15.5080 - val_accuracy: 0.6118\n",
      "Epoch 198/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 8.1426e-07 - accuracy: 1.0000 - val_loss: 15.5217 - val_accuracy: 0.6148\n",
      "Epoch 199/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 7.6566e-07 - accuracy: 1.0000 - val_loss: 15.6620 - val_accuracy: 0.6128\n",
      "Epoch 200/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 7.2130e-07 - accuracy: 1.0000 - val_loss: 15.6421 - val_accuracy: 0.6158\n",
      "Epoch 201/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 6.7377e-07 - accuracy: 1.0000 - val_loss: 15.5517 - val_accuracy: 0.6148\n",
      "Epoch 202/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 6.3152e-07 - accuracy: 1.0000 - val_loss: 15.6769 - val_accuracy: 0.6128\n",
      "Epoch 203/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 5.9548e-07 - accuracy: 1.0000 - val_loss: 15.6725 - val_accuracy: 0.6088\n",
      "Epoch 204/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 5.6153e-07 - accuracy: 1.0000 - val_loss: 15.6086 - val_accuracy: 0.6168\n",
      "Epoch 205/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 5.2850e-07 - accuracy: 1.0000 - val_loss: 15.6513 - val_accuracy: 0.6128\n",
      "Epoch 206/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 4.9371e-07 - accuracy: 1.0000 - val_loss: 15.8313 - val_accuracy: 0.6078\n",
      "Epoch 207/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 4.6618e-07 - accuracy: 1.0000 - val_loss: 15.8696 - val_accuracy: 0.6108\n",
      "Epoch 208/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 4.3705e-07 - accuracy: 1.0000 - val_loss: 15.7891 - val_accuracy: 0.6148\n",
      "Epoch 209/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 4.0816e-07 - accuracy: 1.0000 - val_loss: 15.7684 - val_accuracy: 0.6148\n",
      "Epoch 210/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 3.8423e-07 - accuracy: 1.0000 - val_loss: 15.7599 - val_accuracy: 0.6138\n",
      "Epoch 211/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 3.6465e-07 - accuracy: 1.0000 - val_loss: 15.8591 - val_accuracy: 0.6148\n",
      "Epoch 212/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 3.4046e-07 - accuracy: 1.0000 - val_loss: 16.0026 - val_accuracy: 0.6138\n",
      "Epoch 213/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 3.1987e-07 - accuracy: 1.0000 - val_loss: 15.9544 - val_accuracy: 0.6138\n",
      "Epoch 214/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 3.0270e-07 - accuracy: 1.0000 - val_loss: 15.9789 - val_accuracy: 0.6138\n",
      "Epoch 215/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.8820e-07 - accuracy: 1.0000 - val_loss: 16.1676 - val_accuracy: 0.6088\n",
      "Epoch 216/300\n",
      "4006/4006 [==============================] - 10s 2ms/step - loss: 2.6797e-07 - accuracy: 1.0000 - val_loss: 16.0176 - val_accuracy: 0.6128\n",
      "Epoch 217/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.5431e-07 - accuracy: 1.0000 - val_loss: 16.2362 - val_accuracy: 0.6138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.4032e-07 - accuracy: 1.0000 - val_loss: 16.1465 - val_accuracy: 0.6128\n",
      "Epoch 219/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.2476e-07 - accuracy: 1.0000 - val_loss: 16.1354 - val_accuracy: 0.6098\n",
      "Epoch 220/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.1086e-07 - accuracy: 1.0000 - val_loss: 16.1339 - val_accuracy: 0.6148\n",
      "Epoch 221/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 2.0259e-07 - accuracy: 1.0000 - val_loss: 16.2344 - val_accuracy: 0.6138\n",
      "Epoch 222/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.8857e-07 - accuracy: 1.0000 - val_loss: 16.3353 - val_accuracy: 0.6118\n",
      "Epoch 223/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.7941e-07 - accuracy: 1.0000 - val_loss: 16.3241 - val_accuracy: 0.6108\n",
      "Epoch 224/300\n",
      "4006/4006 [==============================] - 9s 2ms/step - loss: 1.6962e-07 - accuracy: 1.0000 - val_loss: 16.5157 - val_accuracy: 0.6118\n",
      "Epoch 225/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.5873e-07 - accuracy: 1.0000 - val_loss: 16.3189 - val_accuracy: 0.6108\n",
      "Epoch 226/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.5167e-07 - accuracy: 1.0000 - val_loss: 16.4083 - val_accuracy: 0.6118\n",
      "Epoch 227/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.4322e-07 - accuracy: 1.0000 - val_loss: 16.5947 - val_accuracy: 0.6088\n",
      "Epoch 228/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.3751e-07 - accuracy: 1.0000 - val_loss: 16.4995 - val_accuracy: 0.6098\n",
      "Epoch 229/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.2885e-07 - accuracy: 1.0000 - val_loss: 16.3924 - val_accuracy: 0.6138\n",
      "Epoch 230/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.2379e-07 - accuracy: 1.0000 - val_loss: 16.6739 - val_accuracy: 0.6078\n",
      "Epoch 231/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.1766e-07 - accuracy: 1.0000 - val_loss: 16.5162 - val_accuracy: 0.6078\n",
      "Epoch 232/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.1213e-07 - accuracy: 1.0000 - val_loss: 16.5271 - val_accuracy: 0.6088\n",
      "Epoch 233/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 1.0731e-07 - accuracy: 1.0000 - val_loss: 16.5909 - val_accuracy: 0.6068\n",
      "Epoch 234/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 1.0287e-07 - accuracy: 1.0000 - val_loss: 16.5867 - val_accuracy: 0.6118\n",
      "Epoch 235/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 9.8319e-08 - accuracy: 1.0000 - val_loss: 16.7022 - val_accuracy: 0.6078\n",
      "Epoch 236/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 9.5611e-08 - accuracy: 1.0000 - val_loss: 16.5503 - val_accuracy: 0.6118\n",
      "Epoch 237/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 9.2338e-08 - accuracy: 1.0000 - val_loss: 16.6252 - val_accuracy: 0.6098\n",
      "Epoch 238/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 8.9362e-08 - accuracy: 1.0000 - val_loss: 16.5880 - val_accuracy: 0.6088\n",
      "Epoch 239/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 8.7011e-08 - accuracy: 1.0000 - val_loss: 16.8059 - val_accuracy: 0.6088\n",
      "Epoch 240/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 8.4958e-08 - accuracy: 1.0000 - val_loss: 16.6923 - val_accuracy: 0.6098\n",
      "Epoch 241/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 8.4571e-08 - accuracy: 1.0000 - val_loss: 16.7012 - val_accuracy: 0.6068\n",
      "Epoch 242/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 8.0733e-08 - accuracy: 1.0000 - val_loss: 16.8135 - val_accuracy: 0.6108\n",
      "Epoch 243/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.8054e-08 - accuracy: 1.0000 - val_loss: 16.7399 - val_accuracy: 0.6068\n",
      "Epoch 244/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.9632e-08 - accuracy: 1.0000 - val_loss: 16.9001 - val_accuracy: 0.6038\n",
      "Epoch 245/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.6418e-08 - accuracy: 1.0000 - val_loss: 16.8529 - val_accuracy: 0.6118\n",
      "Epoch 246/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 7.4632e-08 - accuracy: 1.0000 - val_loss: 16.7596 - val_accuracy: 0.6078\n",
      "Epoch 247/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 7.4097e-08 - accuracy: 1.0000 - val_loss: 16.9961 - val_accuracy: 0.6108\n",
      "Epoch 248/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 7.2728e-08 - accuracy: 1.0000 - val_loss: 16.9571 - val_accuracy: 0.6038\n",
      "Epoch 249/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.2817e-08 - accuracy: 1.0000 - val_loss: 16.8160 - val_accuracy: 0.6058\n",
      "Epoch 250/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.1389e-08 - accuracy: 1.0000 - val_loss: 16.9466 - val_accuracy: 0.6028\n",
      "Epoch 251/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.0347e-08 - accuracy: 1.0000 - val_loss: 16.9841 - val_accuracy: 0.6068\n",
      "Epoch 252/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 7.0050e-08 - accuracy: 1.0000 - val_loss: 17.0606 - val_accuracy: 0.6098\n",
      "Epoch 253/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.9246e-08 - accuracy: 1.0000 - val_loss: 16.8265 - val_accuracy: 0.6058\n",
      "Epoch 254/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.9544e-08 - accuracy: 1.0000 - val_loss: 16.9935 - val_accuracy: 0.6088\n",
      "Epoch 255/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.8443e-08 - accuracy: 1.0000 - val_loss: 16.9053 - val_accuracy: 0.6078\n",
      "Epoch 256/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.6627e-08 - accuracy: 1.0000 - val_loss: 16.9665 - val_accuracy: 0.6128\n",
      "Epoch 257/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.8264e-08 - accuracy: 1.0000 - val_loss: 17.0158 - val_accuracy: 0.6048\n",
      "Epoch 258/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.5943e-08 - accuracy: 1.0000 - val_loss: 17.0477 - val_accuracy: 0.6038\n",
      "Epoch 259/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.6181e-08 - accuracy: 1.0000 - val_loss: 17.1033 - val_accuracy: 0.6038\n",
      "Epoch 260/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.6657e-08 - accuracy: 1.0000 - val_loss: 17.1141 - val_accuracy: 0.6018\n",
      "Epoch 261/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.4515e-08 - accuracy: 1.0000 - val_loss: 16.9805 - val_accuracy: 0.6108\n",
      "Epoch 262/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.5378e-08 - accuracy: 1.0000 - val_loss: 17.2389 - val_accuracy: 0.6098\n",
      "Epoch 263/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.5318e-08 - accuracy: 1.0000 - val_loss: 17.1380 - val_accuracy: 0.6138\n",
      "Epoch 264/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.4604e-08 - accuracy: 1.0000 - val_loss: 17.0696 - val_accuracy: 0.6078\n",
      "Epoch 265/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2551e-08 - accuracy: 1.0000 - val_loss: 17.1421 - val_accuracy: 0.6038\n",
      "Epoch 266/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.5080e-08 - accuracy: 1.0000 - val_loss: 17.2255 - val_accuracy: 0.6108\n",
      "Epoch 267/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.2194e-08 - accuracy: 1.0000 - val_loss: 17.2890 - val_accuracy: 0.6118\n",
      "Epoch 268/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.3057e-08 - accuracy: 1.0000 - val_loss: 17.0948 - val_accuracy: 0.6108\n",
      "Epoch 269/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.4039e-08 - accuracy: 1.0000 - val_loss: 17.1993 - val_accuracy: 0.6158\n",
      "Epoch 270/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.3652e-08 - accuracy: 1.0000 - val_loss: 17.4793 - val_accuracy: 0.6058\n",
      "Epoch 271/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.3057e-08 - accuracy: 1.0000 - val_loss: 17.1958 - val_accuracy: 0.6068\n",
      "Epoch 272/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.3920e-08 - accuracy: 1.0000 - val_loss: 17.4241 - val_accuracy: 0.6078\n",
      "Epoch 273/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1598e-08 - accuracy: 1.0000 - val_loss: 17.6412 - val_accuracy: 0.6058\n",
      "Epoch 274/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1866e-08 - accuracy: 1.0000 - val_loss: 17.3989 - val_accuracy: 0.6108\n",
      "Epoch 275/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1807e-08 - accuracy: 1.0000 - val_loss: 17.8105 - val_accuracy: 0.6088\n",
      "Epoch 276/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2402e-08 - accuracy: 1.0000 - val_loss: 17.4911 - val_accuracy: 0.6128\n",
      "Epoch 277/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2313e-08 - accuracy: 1.0000 - val_loss: 17.5013 - val_accuracy: 0.6128\n",
      "Epoch 278/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2789e-08 - accuracy: 1.0000 - val_loss: 17.5939 - val_accuracy: 0.6108\n",
      "Epoch 279/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1807e-08 - accuracy: 1.0000 - val_loss: 17.7055 - val_accuracy: 0.6088\n",
      "Epoch 280/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1836e-08 - accuracy: 1.0000 - val_loss: 17.5004 - val_accuracy: 0.6088\n",
      "Epoch 281/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1509e-08 - accuracy: 1.0000 - val_loss: 17.5272 - val_accuracy: 0.6078\n",
      "Epoch 282/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.2699e-08 - accuracy: 1.0000 - val_loss: 17.7432 - val_accuracy: 0.6068\n",
      "Epoch 283/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1896e-08 - accuracy: 1.0000 - val_loss: 17.6246 - val_accuracy: 0.6118\n",
      "Epoch 284/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1777e-08 - accuracy: 1.0000 - val_loss: 17.7926 - val_accuracy: 0.6098\n",
      "Epoch 285/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1450e-08 - accuracy: 1.0000 - val_loss: 17.5436 - val_accuracy: 0.6158\n",
      "Epoch 286/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.2759e-08 - accuracy: 1.0000 - val_loss: 17.7982 - val_accuracy: 0.6128\n",
      "Epoch 287/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.0884e-08 - accuracy: 1.0000 - val_loss: 17.8141 - val_accuracy: 0.6048\n",
      "Epoch 288/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2015e-08 - accuracy: 1.0000 - val_loss: 17.4421 - val_accuracy: 0.6098\n",
      "Epoch 289/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1241e-08 - accuracy: 1.0000 - val_loss: 17.7950 - val_accuracy: 0.6108\n",
      "Epoch 290/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.0944e-08 - accuracy: 1.0000 - val_loss: 17.8846 - val_accuracy: 0.6098\n",
      "Epoch 291/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1717e-08 - accuracy: 1.0000 - val_loss: 17.5764 - val_accuracy: 0.6108\n",
      "Epoch 292/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.1896e-08 - accuracy: 1.0000 - val_loss: 17.7011 - val_accuracy: 0.6128\n",
      "Epoch 293/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1479e-08 - accuracy: 1.0000 - val_loss: 17.7380 - val_accuracy: 0.6108\n",
      "Epoch 294/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.3771e-08 - accuracy: 1.0000 - val_loss: 17.8612 - val_accuracy: 0.6048\n",
      "Epoch 295/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.1479e-08 - accuracy: 1.0000 - val_loss: 17.6786 - val_accuracy: 0.6088\n",
      "Epoch 296/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2818e-08 - accuracy: 1.0000 - val_loss: 17.4937 - val_accuracy: 0.6098\n",
      "Epoch 297/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.2878e-08 - accuracy: 1.0000 - val_loss: 18.0069 - val_accuracy: 0.6098\n",
      "Epoch 298/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.2878e-08 - accuracy: 1.0000 - val_loss: 17.2546 - val_accuracy: 0.6118\n",
      "Epoch 299/300\n",
      "4006/4006 [==============================] - 11s 3ms/step - loss: 6.3235e-08 - accuracy: 1.0000 - val_loss: 18.0315 - val_accuracy: 0.6028\n",
      "Epoch 300/300\n",
      "4006/4006 [==============================] - 10s 3ms/step - loss: 6.3295e-08 - accuracy: 1.0000 - val_loss: 17.8696 - val_accuracy: 0.6088\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 300, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 1.0\n",
      "train acc: 0.6087824106216431\n"
     ]
    }
   ],
   "source": [
    "# 1) create loss and accuracy plots\n",
    "_, train_acc = model2.evaluate(X_train, Y_train, verbose=0)\n",
    "_, test_acc = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "# 1.1) plot loss during training\n",
    "pyplot.figure(1, (9,9))\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# 1.2) plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "print('train acc:', train_acc)\n",
    "print('train acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 1s 1ms/step\n",
      "[[24  5  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [12 16  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 2  0 14 17  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  2  6 21  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  2  2 46  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0 32  1  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  2  9  2  3  2  1  1  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  2  3 33  3  2  0  1  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  2  3 28  6  4  1  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  2  1 25  6  9  5  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  6 23  3  1  0  0  0  0  0  0  1  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1  5  7 14 13  6  0  2  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1  4  2  7 15  2  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  1 39  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 28  5  3  4  1  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  4 21  3  8  1  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  4  4 16 15  6  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  0  0  0  2  2  3 28  4  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  3  4  1  8 12  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  9  3  1  7 10\n",
      "   3  3]\n",
      " [ 0  0  0  0  0  1  1  0  0  0  1  0  1  1  0  0  0  0  0  8  6  2  2  5\n",
      "   2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40  0  0\n",
      "   0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  2 19  8\n",
      "   1  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  5  0  1 14 28\n",
      "   4  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  1  1  2  1\n",
      "  22  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "   0 42]]\n"
     ]
    }
   ],
   "source": [
    "# print out confusion matrix of predicted classifications\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model2.predict(X_test, verbose=1)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "Y_test = np.argmax(Y_test, axis=-1)\n",
    "output_confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(output_confusion_matrix)\n",
    "np.savetxt(\"model4_output_4_ext_spread.csv\", output_confusion_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store train and dev set accuracy at each epoch\n",
    "np.savetxt(\"model4_output_4_loss_ext_spread.csv\", history.history['loss'], delimiter=\",\")\n",
    "np.savetxt(\"model4_output_4_val_loss_ext_spread.csv\", history.history['val_loss'], delimiter=\",\")\n",
    "np.savetxt(\"model4_output_4_accuracy_ext_spread.csv\", history.history['accuracy'], delimiter=\",\")\n",
    "np.savetxt(\"model4_output_4_val_accuracy_ext_spread.csv\", history.history['val_accuracy'], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
